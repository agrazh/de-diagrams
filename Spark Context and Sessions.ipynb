{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "406db2d5-dd03-4006-9312-d66191f3c4c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf95dfa-f8d5-4456-8ee0-4d42149919f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# create SparkConf object\n",
    "conf = SparkConf().setAppName(\"myApp\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)  # or SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.collect()\n",
    "\n",
    "\n",
    "# get SparkContext settings in Spark 1.x, 2.x\n",
    "sc.getConf().getAll()\n",
    "sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "# stop SparkContext and restart the driver\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23300ed4-c7f5-422d-b611-528eb2a3dc1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7b8205-fa9b-48e5-b709-a31922ed9f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 'Databricks Shell'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create Sessions\n",
    "spark1 = SparkSession.builder.appName(\"Session1\").getOrCreate()\n",
    "spark2 = SparkSession.builder.appName(\"Session2\") \\\n",
    "    .config(\"spark.some.setting\", \"value2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# get SparkContext settings in Spark 2.x\n",
    "spark1.sparkContext.getConf().getAll()\n",
    "spark2.sparkContext.getConf().get(\"spark.app.name\")\n",
    "\n",
    "# stop all sessions and SparkContext and restart the driver\n",
    "spark1.stop()  # == spark1.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b78e4a1-e055-4b77-aafd-6d6358a6b651",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Two Sessions with distinct tables [Demo]\n",
    "\n",
    "Session: `spark1` , `spark2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5319181d-ad59-405d-bb8b-3d3784ce91cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@16d154f4\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@16d154f4\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// crate new sessions\n",
    "val spark1 = spark.newSession()\n",
    "val spark2 = spark.newSession()\n",
    "\n",
    "// get \"spark1\" and \"spark2\" session ids. They are different\n",
    "println(spark1)\n",
    "println(spark2)\n",
    "\n",
    "// get session contexts. They are the same\n",
    "println(spark.sparkContext)\n",
    "println(new_spark.sparkContext)\n",
    "\n",
    "// create tables in both sessions\n",
    "val df = spark1.range(100).toDF()\n",
    "df.createOrReplaceTempView(\"vtable_1\")\n",
    "spark2.range(100).toDF().createOrReplaceTempView(\"vtable_2\")\n",
    "\n",
    "// list all tables in both sessions\n",
    "spark1.catalog.listTables.show()  // or spark.sqlContext.sql(\"show tables\").show()\n",
    "spark2.catalog.listTables.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Context and Sessions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
