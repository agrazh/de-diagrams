{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae16ade4-dd1f-4d1f-a6f9-7e5a5eaccbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7373266c-b484-4465-8210-eceabe99d057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"myApp\").setMaster(\"local[*]\")  // use all available logical CPU cores\n",
    "val sc = SparkContext.getOrCreate(conf)  // or `new SparkContext(conf)`\n",
    "\n",
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "rdd.collect()  // Array[Int] = Array(1, 2, 3, 4, 5)\n",
    "\n",
    "// get SparkContext settings in Spark 1.x, 2.x\n",
    "sc.getConf.getAll\n",
    "sc.getConf.get(\"spark.app.name\")\n",
    "\n",
    "// stop SparkContext and restart the driver\n",
    "sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd883433-7cf7-4147-8eb6-180d528c2a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2554b292-fc5f-45e1-b74d-a0c4e541102b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.{SparkSession}\n",
    "\n",
    "// create Sessions\n",
    "val spark1 = SparkSession.builder.appName(\"Session1\") // or `SparkSession.builder.appName(\"Session1\").getOrCreate()`\n",
    "val spark2 = SparkSession.builder.appName(\"Session2\")\n",
    "  .config(\"spark.some.setting\", \"value2\")\n",
    "  .config(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "// Get SparkContext settings in Spark 2.x\n",
    "// For some reason this does not work with `spark1` and `spark2` but works with spark.newSession() (see in the next paragraph)\n",
    "spark.sparkContext.getConf.getAll\n",
    "spark.sparkContext.getConf.get(\"spark.app.name\")\n",
    "\n",
    "// stop the underlying SparkContext and both sessions `spark1`, `spark2`\n",
    "spark1.stop()  // or `spark1.sparkContext.stop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f3d7c2-5339-47fb-88c0-01fa194043e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Two Sessions with distinct tables [Demo]\n",
    "\n",
    "Session: `spark1` , `spark2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d3e57c-96be-4a5a-aebd-329e54be259a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// create new sessions\n",
    "val spark1 = spark.newSession()\n",
    "val spark2 = spark.newSession()\n",
    "\n",
    "// get SparkContext settings in Spark 2.x\n",
    "spark1.sparkContext.getConf.getAll\n",
    "spark2.sparkContext.getConf.getAll\n",
    "spark1.sparkContext.getConf.get(\"spark.app.name\")\n",
    "spark2.sparkContext.getConf.get(\"spark.app.name\")\n",
    "\n",
    "// get \"spark1\" and \"spark2\" session ids - they are different\n",
    "println(spark1)\n",
    "println(spark2)\n",
    "\n",
    "// get sessions context ids - they are the same\n",
    "println(spark1.sparkContext)\n",
    "println(spark2.sparkContext)\n",
    "\n",
    "// create tables in both sessions\n",
    "spark1.range(100).toDF().createOrReplaceTempView(\"vtable_1\")\n",
    "spark2.range(100).toDF().createOrReplaceTempView(\"vtable_2\")\n",
    "\n",
    "// list all tables in both sessions\n",
    "spark1.catalog.listTables.show()  // or `spark1.sqlContext.sql(\"show tables\").show()`\n",
    "spark2.catalog.listTables.show()  // or `spark2.sql(\"show tables\").show()`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Context and Sessions (Sc)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
